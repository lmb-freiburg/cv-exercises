{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration of a End-of-Arm Camera\n",
    "\n",
    "This exercise will describe how to calibrate a end-of-arm camera using marker-board detection.\n",
    "The excercise is divided into the following steps.\n",
    "\n",
    "1. Load recorded images.\n",
    "2. Show marker detection results.\n",
    "3. Solve for the calibration.\n",
    "4. Analysis of results.\n",
    "\n",
    "First lets have a look at the data.\n",
    "\n",
    "**Note about open3d**: we use [Open3D](http://www.open3d.org/) for this exercise.\n",
    "\n",
    "Install it e.g. with pip as `pip install open3d`.\n",
    "\n",
    "If you get segmentation fault / kernel crashed, try running it in python 3.8 with a new environment:\n",
    "\n",
    "```bash\n",
    "conda create -n o3d python=3.8 -y\n",
    "conda activate o3d\n",
    "pip install open3d numpy matplotlib scipy pillow\n",
    "pip install \"notebook<7\"\n",
    "```\n",
    "\n",
    "If you are on windows and the installation fails with \"rust not found\" install it:\n",
    "go to https://forge.rust-lang.org/infra/other-installation-methods.html and\n",
    "download https://static.rust-lang.org/rustup/dist/i686-pc-windows-gnu/rustup-init.exe\n",
    "\n",
    "\n",
    "Unfortunately the visualization requires OpenGL, which fails for setups like ours, where the visualization and the code run on separate machines. A minimal working solution (with drawbacks in quality) is to start a jupyter notebook on one of the pool machines (not the login) and use vscode for port-forwarding it to your local browser.\n",
    "*The recommended way is to run everything on your local machine*\n",
    "\n",
    "**Note about the interactive matlotlib viewer:**  As of 2024-12, interactive matplotlib only works with older notebook versions so downgrade your versions with\n",
    "`pip install \"notebook<7\"`. \n",
    "Start it in the command line via` jupyter notebook --port xxxx`.\n",
    "Use the vs-code pop up to open jupyter in your local server.\n",
    "If the interactive viewer still does not work you may have to restart the notebook and try it with `%matplotlib widget` instead of `%matplotlib notebook`\n",
    "\n",
    "Note:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# %matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename=\"calibration_ls.png\", width=1000)  # image adapted from torsteinmyhre.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test open3d visualization\n",
    "import open3d as o3d\n",
    "mesh_frames = []\n",
    "mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5)\n",
    "mesh_frames.append(mesh_frame)\n",
    "o3d.visualization.draw_geometries(mesh_frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise want to do eye-in-hand extrinsic calibration. In this setup, a camera (eye) is mounted at end of a robot arm. The end of the robot arm is called Tool Center Point (TCP) because this is where tools are mounted. We would like to find the transformation `T_cam_tcp`. We do this calibration by moving the robot and taking several pictures of the marker board.\n",
    "\n",
    "In the setup, the marker board, and the base of the robot are static, this means that `T_cam_tcp` as well as `T_robot_marker` are fixed. For each view recorded the robot arm is in a different location, which changes the position of the marker board as seen by the camera. This means that `T_tcp_robot` and `T_cam_marker` change for each view. Calibration works by finding a loop in these poses, and solving for `T_cam_tcp`. This can be done in several ways, in this exercise we present two methods, a simple least-squares optimization and the peak-martin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import io\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from scipy.spatial.transform import Rotation\n",
    "from PIL import Image\n",
    "\n",
    "import open3d as o3d\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "np.set_printoptions(suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code will download the data. Alternatively you can find it in `/project/.../shared-data/marker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_path = Path(\".\")\n",
    "if not (target_path / \"marker\").is_dir():\n",
    "    url = \"https://lmb.informatik.uni-freiburg.de/lectures/computer_vision_I/exercisedata/marker.tar\"\n",
    "    print(f\"Downloading {url}\")\n",
    "    res = requests.get(url, timeout=30)\n",
    "    data = res.content\n",
    "    data_io = io.BytesIO(data)\n",
    "    print(f\"Extracting...\")\n",
    "    with tarfile.TarFile(fileobj=data_io, mode=\"r\") as tar:\n",
    "        tar.extractall(path=target_path.as_posix())\n",
    "    print(f\"Done\")\n",
    "else:\n",
    "    print(f\"Folder exists: {target_path.as_posix()}/marker\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Recorded Images\n",
    "\n",
    "This cell defines a data class that loads images and data from files.\n",
    "\n",
    "Please complete the `get_projection_matrix` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViewLoader:\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "        assert os.path.isdir(base_path)\n",
    "        files = sorted(os.listdir(self.base_path))\n",
    "        files = [f for f in files if (f.startswith(\"rgb_\") and f.endswith(\".png\"))]\n",
    "        self.max_idx = int(files[-1].replace(\"rgb_\", \"\").replace(\".png\", \"\"))\n",
    "        print(f\"Loaded {self.max_idx+1} images.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_idx + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_rgbdp(idx)\n",
    "\n",
    "    def get_info(self):\n",
    "        # read given parameters from info.json\n",
    "        info_path = os.path.join(self.base_path, \"info.json\")\n",
    "        with open(info_path, \"rb\") as f_obj:\n",
    "            info = json.load(f_obj)\n",
    "        return info\n",
    "\n",
    "    def get_intrinsics(self):\n",
    "        # get intrintic camera matrix\n",
    "        info = self.get_info()\n",
    "        calib = info[\"camera\"][\"calibration\"]\n",
    "        return calib\n",
    "\n",
    "    def get_K(self):\n",
    "        calib = self.get_intrinsics()\n",
    "        cam_intrinsic = np.eye(3)\n",
    "        cam_intrinsic[0, 0] = calib[\"fx\"]\n",
    "        cam_intrinsic[1, 1] = calib[\"fy\"]\n",
    "        cam_intrinsic[0, 2] = calib[\"ppx\"]\n",
    "        cam_intrinsic[1, 2] = calib[\"ppy\"]\n",
    "        return cam_intrinsic\n",
    "\n",
    "    def get_robot_pose(self, idx, return_dict=False):\n",
    "        # read robot pose, convert to 4x4 homogeneous format\n",
    "        # returns T_robot_tcp\n",
    "        pose_file = os.path.join(self.base_path, \"pose_{0:04d}.json\".format(idx))\n",
    "        with open(pose_file, \"rb\") as f_obj:\n",
    "            pose = json.load(f_obj)\n",
    "\n",
    "        # pose contains position xyz, rotation xyz and depth (7 parameters)\n",
    "        pose_m = np.eye(4)\n",
    "        pose_m[:3, :3] = Rotation.from_euler(\n",
    "            \"xyz\", [pose[x] for x in [\"rot_x\", \"rot_y\", \"rot_z\"]]\n",
    "        ).as_matrix()\n",
    "        pose_m[:3, 3] = [pose[x] for x in [\"x\", \"y\", \"z\"]]\n",
    "        if return_dict:\n",
    "            return pose_m, pose\n",
    "        else:\n",
    "            return pose_m\n",
    "\n",
    "    def get_rgb_file(self, idx):\n",
    "        # read RGB image\n",
    "        rgb_file = os.path.join(self.base_path, \"rgb_{0:04d}.png\".format(idx))\n",
    "        return rgb_file\n",
    "\n",
    "    def get_depth_file(self, idx):\n",
    "        # read depth image\n",
    "        depth_file = os.path.join(self.base_path, \"depth_{0:04d}.png\".format(idx))\n",
    "        return depth_file\n",
    "\n",
    "    def get_rgbdp(self, idx):\n",
    "        # get RGB, scaled-depth, robot pose\n",
    "        rgb_file = self.get_rgb_file(idx)\n",
    "        rgb = np.asarray(Image.open(rgb_file))\n",
    "\n",
    "        pose_m, pose_d = self.get_robot_pose(idx, True)\n",
    "\n",
    "        # depth images here are saved 16-bit grayscale for efficient storage\n",
    "        # depth_scaling is the factor required to convert depth to meters\n",
    "        depth_file = self.get_depth_file(idx)\n",
    "        depth_scaling = pose_d[\"depth_scaling\"]\n",
    "        depth = np.asarray(Image.open(depth_file), dtype=np.float32) * depth_scaling\n",
    "        return rgb, depth, pose_m\n",
    "\n",
    "    def get_cam_pose(self, idx, marker_dir=\"pose_marker_one\"):\n",
    "        # get camera pose in 4x4 homogeneous format\n",
    "        # these are the results from marker detection: T_cam_marker\n",
    "        marker_dir = os.path.join(self.base_path, marker_dir)\n",
    "        fn = \"{0:08d}.json\".format(idx)\n",
    "        pose_fn = os.path.join(marker_dir, fn)\n",
    "        with open(pose_fn, \"r\") as fo:\n",
    "            T_cam_marker = np.array(json.load(fo))\n",
    "        return T_cam_marker\n",
    "\n",
    "    def get_projection_matrix(self):\n",
    "        # returns a 3x4 projection matrix using the intrinsics\n",
    "        # it projects a 3d point in in homogeneous coordinates\n",
    "        # to a 2d point in homogeneous coordinates\n",
    "        # assuming the camera frame and world frame are aligned\n",
    "\n",
    "        # START TODO #################\n",
    "        # intr = self.get_intrinsics()\n",
    "        # ...\n",
    "        raise NotImplementedError\n",
    "        # END TODO ###################\n",
    "        assert cam_mat.shape == (3, 4)\n",
    "        return cam_mat\n",
    "\n",
    "    def project(self, X):\n",
    "        if X.shape[0] == 3:\n",
    "            # convert coordinate to homogeneous if it is euclidean\n",
    "            if len(X.shape) == 1:\n",
    "                X = np.append(X, 1)\n",
    "            else:\n",
    "                X = np.concatenate([X, np.ones((1, X.shape[1]))], axis=0)\n",
    "\n",
    "        # project a point from the 3d camera coordinate system into\n",
    "        # the 2d camera frame. (in homogeneous coordinates)\n",
    "        # given the camera projection matrix\n",
    "        x = self.get_projection_matrix() @ X\n",
    "\n",
    "        # convert homogeneous to euclidean and check if it is inside the image boundaries\n",
    "        result = np.round(x[0:2] / x[2]).astype(int)\n",
    "        width, height = self.get_intrinsics()[\"width\"], self.get_intrinsics()[\"height\"]\n",
    "        if not (0 <= result[0] < width and 0 <= result[1] < height):\n",
    "            log.warning(\"Projected point outside of image bounds\")\n",
    "        return result[0], result[1]\n",
    "\n",
    "\n",
    "vl = ViewLoader(base_path=\"marker\")\n",
    "print(\"camera calibration:\")\n",
    "camera_calibration = vl.get_K()\n",
    "K = np.array(camera_calibration)\n",
    "print(K.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, Layout, interact\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "image, depth, pose = vl.get_rgbdp(0)\n",
    "line = ax.imshow(np.asarray(image))\n",
    "ax.set_axis_off()\n",
    "\n",
    "\n",
    "def update(w):\n",
    "    image, depth, pose = vl.get_rgbdp(w)\n",
    "    line.set_data(np.asarray(image))\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "\n",
    "slider_w = widgets.IntSlider(min=0, max=len(vl) - 1, step=1, value=0, layout=Layout(width=\"70%\"))\n",
    "interact(update, w=slider_w)\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Show Marker Detection Results.\n",
    "\n",
    "To simplify things marker detection has already been run. Next we want to verify its results.\n",
    "Do this by drawing a coordinate frame into each image for which we have detection results.\n",
    "The coordinate frame should have axis lengths of 10cm, with x=red, y=green, and z=blue.\n",
    "This can be done using `PIL.ImageDraw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "\n",
    "def show_marker_pose(image, T_cam_marker):\n",
    "    \"\"\"\n",
    "    draw the coordinate frame into each image for which we have detection results\n",
    "\n",
    "    Arguments:\n",
    "        image: image as numpy.ndarray\n",
    "        T_cam_marker: shape (4, 4), transform from marker into camera\n",
    "            x_cam = T_cam_marker @ x_marker\n",
    "    Returns:\n",
    "        im: image (should be PIL.Image.Image)\n",
    "    \"\"\"\n",
    "    print(T_cam_marker)\n",
    "    # START TODO #################\n",
    "    # using PIL.ImageDraw\n",
    "    # 1. Define 4 points in 3D homogeneous coordinates: <x, y, z, center>\n",
    "    #    (The center of the coordinate system and a point for each of 3 axes.)\n",
    "    #    Note that the 3D here are specified in meters.\n",
    "    # 2. Transform the <x, y, z, center> coordinates into the camera frame\n",
    "    # 3. Project the homogeneous coordinates <cam_x, cam_y, cam_z, center>\n",
    "    #    to the camera image (euclidean)\n",
    "    # 4. Draw one line for each axis\n",
    "    raise NotImplementedError\n",
    "    # END TODO ###################\n",
    "\n",
    "    # type(im) should be PIL.Image.Image\n",
    "    return im\n",
    "\n",
    "\n",
    "image, depth, robot_pose = vl.get_rgbdp(1)\n",
    "T_cam_marker = vl.get_cam_pose(1)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "image_m = show_marker_pose(image, T_cam_marker)\n",
    "line = ax.imshow(np.asarray(image_m))\n",
    "ax.set_axis_off()\n",
    "\n",
    "\n",
    "def update(w):\n",
    "    image, depth, pose = vl.get_rgbdp(w)\n",
    "    try:\n",
    "        T_cam_marker = vl.get_cam_pose(w)\n",
    "    except FileNotFoundError:\n",
    "        print(\"No pose estimation.\")\n",
    "        line.set_data(np.asarray(image))\n",
    "        return\n",
    "    image_m = show_marker_pose(image, T_cam_marker)\n",
    "    line.set_data(np.asarray(image_m))\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "\n",
    "slider_w = widgets.IntSlider(min=0, max=len(vl) - 1, step=1, value=0, layout=Layout(width=\"70%\"))\n",
    "interact(update, w=slider_w)\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tcp_marker_lists(m_dir=\"pose_marker_one\"):\n",
    "    T_robot_tcp_list = []\n",
    "    T_cam_marker_list = []\n",
    "    for i in range(len(vl)):\n",
    "        try:\n",
    "            robot_pose = vl.get_robot_pose(i)\n",
    "            cam_pose = vl.get_cam_pose(i, marker_dir=m_dir)\n",
    "        except (FileNotFoundError, ValueError):\n",
    "            continue\n",
    "        T_robot_tcp_list.append(robot_pose)\n",
    "        T_cam_marker_list.append(cam_pose)\n",
    "\n",
    "    return np.array(T_robot_tcp_list), np.array(T_cam_marker_list)\n",
    "\n",
    "\n",
    "# draw the markers in 3d\n",
    "import open3d as o3d\n",
    "\n",
    "plot_o3d = True\n",
    "# plot_o3d = False\n",
    "if plot_o3d:\n",
    "    T_robot_tcp_list, T_cam_marker_list = get_tcp_marker_lists()\n",
    "    # create one big frame to show the absolute zero point of the plot\n",
    "    mesh_frames = []\n",
    "    mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5)\n",
    "    mesh_frames.append(mesh_frame)\n",
    "\n",
    "    for T_robot_tcp, T_cam_marker in zip(T_robot_tcp_list, T_cam_marker_list):\n",
    "        # create coordinate frames around the zero point and transform them\n",
    "        # to visualize the transforms\n",
    "\n",
    "        # # T_robot_tcp shows where the tool is relative to the robot base (small arrows)\n",
    "        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1)\n",
    "        mesh_frame.transform(T_robot_tcp)\n",
    "        mesh_frames.append(mesh_frame)\n",
    "\n",
    "        # # you can also view the robot base relative to the tool\n",
    "        # T_tcp_robot = np.linalg.inv(T_robot_tcp)\n",
    "        # mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1)\n",
    "        # mesh_frame.transform(T_tcp_robot)\n",
    "        # mesh_frames.append(mesh_frame)\n",
    "\n",
    "        # # shows where the cam is relative to the marker (big arrows)\n",
    "        T_marker_cam = np.linalg.inv(T_cam_marker)\n",
    "        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.2)\n",
    "        mesh_frame.transform(T_marker_cam)\n",
    "        mesh_frames.append(mesh_frame)\n",
    "\n",
    "        # # shows where the marker is relative to the cam\n",
    "        # mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.20)\n",
    "        # mesh_frame.transform(T_cam_marker)\n",
    "        # mesh_frames.append(mesh_frame)\n",
    "\n",
    "    # Draw the geometries\n",
    "    o3d.visualization.draw_geometries(mesh_frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Solve for the Calibration\n",
    "\n",
    "We will try to find the calibration using two different methods, via least squares optimization and using the park martin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_matrix(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: is our optimization target, a vector of shape (9,).\n",
    "            [0:3] position transform of T_cam_tcp\n",
    "            [3:6] rotation transform of T_cam_tcp\n",
    "                (angles to rotate around xyz axes)\n",
    "            [6:9] position transform of T_robot_marker (discarded here)\n",
    "\n",
    "    Returns:\n",
    "        transformation matrix T_cam_tcp shape (4, 4)\n",
    "\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.Rotation.from_euler.html\n",
    "    \"\"\"\n",
    "    mat = np.eye(4)\n",
    "    mat[:3, 3] = x[0:3]\n",
    "    mat[:3, :3] = Rotation.from_euler(\"xyz\", x[3:6]).as_matrix()\n",
    "    return mat\n",
    "\n",
    "\n",
    "def pprint(arr):\n",
    "    return np.array2string(arr.round(5), separator=\", \")\n",
    "\n",
    "\n",
    "def matrix_to_pos_orn(mat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mat: 4x4 homogeneous transformation\n",
    "\n",
    "    Returns:\n",
    "        position: np.array of shape (3,),\n",
    "        orientation: np.array of shape (4,) -> quaternion xyzw\n",
    "\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.Rotation.as_quat.html\n",
    "    \"\"\"\n",
    "    orn = Rotation.from_matrix(mat[:3, :3]).as_quat()\n",
    "    pos = mat[:3, 3]\n",
    "    return pos, orn\n",
    "\n",
    "\n",
    "def pose_error(T_tcp_cam, T_robot_tcp_list, T_cam_marker_list):\n",
    "    \"\"\"\n",
    "    returns position error tuple for each entry\n",
    "\n",
    "    Explanation: We know that T_robot_marker is constant.\n",
    "    Given our calibrated T_tcp_cam we calculate T_robot_marker for each pair\n",
    "    of robot position T_robot_tcp and marker detection result T_cam_marker.\n",
    "    Now assuming perfect marker detection, robot position and calibration,\n",
    "    the calculated T_robot_marker should be the same for all pairs.\n",
    "    Therefore we can use derivations of the mean pose as pose error.\n",
    "\n",
    "    Args:\n",
    "        T_tcp_cam: Transform from camera to tool center shape (4, 4)\n",
    "        T_robot_tcp_list: List of robot poses i.e. transform from tool center\n",
    "            point to robot base, shape (N, 4, 4)\n",
    "        T_cam_marker_list: List of marker measurements i.e. transform from\n",
    "            marker to camera, shape (N, 4, 4)\n",
    "    Returns:\n",
    "        error: cartesian error (N, 3)\n",
    "    \"\"\"\n",
    "    T_robot_marker_list = []\n",
    "    for T_robot_tcp, T_cam_marker in zip(T_robot_tcp_list, T_cam_marker_list):\n",
    "        T_robot_marker = T_robot_tcp @ T_tcp_cam @ T_cam_marker\n",
    "        T_robot_marker_list.append(T_robot_marker)\n",
    "\n",
    "    poses = np.array(T_robot_marker_list)[:, :3, 3]\n",
    "    err = poses - np.mean(poses, axis=0)\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least-Squares Optimization\n",
    "\n",
    "`T_robot_marker` and `T_cam_tcp` are fixed but unknown, so these will be used as variables that have to be optimized.\n",
    "From these values compute the predicted `T_cam_marker_pred` and compare it to `T_cam_marker_obs`.\n",
    "Since we have enough measurements, we can do the comparison based only on position values (i.e. it is enough to predict how the origin moves between the coordinate systems), so `T_robot_marker` only uses position components.\n",
    "Optimize the function using `scipy.optimize.least_squares`.\n",
    "\n",
    "`T_cam_marker = T_cam_tcp @ T_tcp_robot @ T_robot_marker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import least_squares, minimize\n",
    "\n",
    "\n",
    "def compute_residuals_gripper_cam(x, T_robot_tcp_list, T_cam_marker_list):\n",
    "    \"\"\"\n",
    "    Calculate predicted positional transform from marker to camera using\n",
    "    T_cam_tcp, T_robot_marker and T_tcp_robot.\n",
    "    Compare these predicted values with the observed positional transform\n",
    "    T_cam_marker to calculate and returns the residuals\n",
    "\n",
    "    Args:\n",
    "        x: is our optimization target, a vector of shape (9,).\n",
    "            [0:3] position transform of T_cam_tcp\n",
    "            [3:6] rotation transform of T_cam_tcp\n",
    "                (angles to rotate around xyz axes)\n",
    "            [6:9] position transform of T_robot_marker\n",
    "        T_robot_tcp_list: List of robot poses i.e. transform from tool center\n",
    "            point to robot base, each entry shape (4, 4)\n",
    "        T_cam_marker_list: List of marker measurements i.e. transform from\n",
    "            marker to camera, each entry shape (4, 4)\n",
    "\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.Rotation.from_euler.html\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html\n",
    "    \"\"\"\n",
    "    T_robot_marker = np.array([*x[6:], 1])  # shape (4, )\n",
    "    T_cam_tcp = vec_to_matrix(x)  # shape (4, 4)\n",
    "\n",
    "    # START TODO #################\n",
    "    raise NotImplementedError\n",
    "    # END TODO ###################\n",
    "\n",
    "    # len(residuals) will be 144 = 48 samples * 3 (x,y,z), for least-squares optimization\n",
    "    return residuals\n",
    "\n",
    "\n",
    "def calibrate_gripper_cam_ls(T_robot_tcp_list, T_cam_marker_list):\n",
    "    # use scipy least squares to optimize the above function and return the calibration\n",
    "    # START TODO #################\n",
    "    raise NotImplementedError\n",
    "    # END TODO ###################\n",
    "\n",
    "    assert T_tcp_cam.shape == (4, 4)\n",
    "    return T_tcp_cam\n",
    "\n",
    "\n",
    "T_robot_tcp_list, T_cam_marker_list = get_tcp_marker_lists()\n",
    "T_tcp_cam = calibrate_gripper_cam_ls(T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls = pose_error(T_tcp_cam, T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls_s = np.sum(err_ls**2, axis=1)\n",
    "# analyze median error since that is more robust to outliers than mean error\n",
    "print(\"median error ls\", np.median(err_ls_s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to slighty improve the result using differential evolution\n",
    "# nothing required here.\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html\n",
    "\n",
    "\n",
    "def calculate_error(T_tcp_cam, T_robot_tcp_list, T_cam_marker_list, inliers=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        T_tcp_cam: Transform from camera to tool center shape (4,4)\n",
    "        T_robot_tcp_list: List of robot poses i.e. transform from tool center\n",
    "            point to robot base, shape (N, 4, 4)\n",
    "        T_cam_marker_list: List of marker measurements i.e. transform from\n",
    "            marker to camera, shape (N, 4, 4)\n",
    "    Returns:\n",
    "        scalar error for each entry\n",
    "    \"\"\"\n",
    "    T_robot_marker_list = []\n",
    "    for T_robot_tcp, T_cam_marker in zip(T_robot_tcp_list, T_cam_marker_list):\n",
    "        T_robot_marker = T_robot_tcp @ T_tcp_cam @ T_cam_marker\n",
    "        T_robot_marker_list.append(T_robot_marker)\n",
    "\n",
    "    poses = np.array(T_robot_marker_list)[:, :3, 3]\n",
    "    if inliers is None:\n",
    "        mean_pose = np.mean(poses, axis=0)\n",
    "    else:\n",
    "        mean_pose = np.mean(poses[inliers], axis=0)\n",
    "    err = np.sum((poses - mean_pose) ** 2, axis=1)\n",
    "    return err\n",
    "\n",
    "\n",
    "import scipy\n",
    "\n",
    "\n",
    "def calibrate_gripper_cam_de(T_robot_tcp_list, T_cam_marker_list):\n",
    "    # optimize using least squares as before\n",
    "    x0 = np.array([0, 0, 0, 0, 0, 0, 0, 0, -0.1])\n",
    "    result = least_squares(\n",
    "        fun=compute_residuals_gripper_cam,\n",
    "        x0=x0,\n",
    "        method=\"lm\",\n",
    "        args=(T_robot_tcp_list, T_cam_marker_list),\n",
    "    )\n",
    "\n",
    "    x0 = result.x\n",
    "    bounds = [(x - 0.0001, x + 0.0001) for x in x0]\n",
    "\n",
    "    def func(x, *args):\n",
    "        T_cam_tcp = vec_to_matrix(x)\n",
    "        return calculate_error(T_cam_tcp, *args).mean()\n",
    "\n",
    "    result2 = scipy.optimize.differential_evolution(\n",
    "        func=func,\n",
    "        bounds=bounds,\n",
    "        args=(T_robot_tcp_list, T_cam_marker_list),\n",
    "        tol=1e-11,\n",
    "    )\n",
    "    T_tcp_cam = np.linalg.inv(vec_to_matrix(result2.x))\n",
    "    # print(result.x)\n",
    "    # print(bounds)\n",
    "    return T_tcp_cam\n",
    "\n",
    "\n",
    "T_tcp_cam = calibrate_gripper_cam_de(T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls = pose_error(T_tcp_cam, T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls_s = np.sum(err_ls**2, axis=1)\n",
    "print(\"median error ls\", np.median(err_ls_s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Park-Martin Calibration\n",
    "\n",
    "Have a look here for the description of the Park-Martin method.\n",
    "\n",
    "https://www.torsteinmyhre.name/snippets/robcam_calibration.html\n",
    "\n",
    "We are trying to solve the following equation: `AX = XB`. The website shows how to create the lists `As`,`Bs`.\n",
    "This is an example for another method to solve the problem posed in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def log_func(R):\n",
    "    # Rotation matrix logarithm\n",
    "    theta = np.arccos((R[0, 0] + R[1, 1] + R[2, 2] - 1.0) / 2.0)\n",
    "    return (\n",
    "        np.array([R[2, 1] - R[1, 2], R[0, 2] - R[2, 0], R[1, 0] - R[0, 1]])\n",
    "        * theta\n",
    "        / (2 * np.sin(theta))\n",
    "    )\n",
    "\n",
    "\n",
    "def invsqrt(mat):\n",
    "    u, s, v = np.linalg.svd(mat)\n",
    "    return u.dot(np.diag(1.0 / np.sqrt(s))).dot(v)\n",
    "\n",
    "\n",
    "def calibrate(A, B):\n",
    "    # transform pairs A_i, B_i\n",
    "    N = len(A)\n",
    "    M = np.zeros((3, 3))\n",
    "    for i in range(N):\n",
    "        Ra, Rb = A[i][0:3, 0:3], B[i][0:3, 0:3]\n",
    "        M += np.outer(log_func(Rb), log_func(Ra))\n",
    "\n",
    "    Rx = np.dot(invsqrt(np.dot(M.T, M)), M.T)\n",
    "\n",
    "    C = np.zeros((3 * N, 3))\n",
    "    d = np.zeros((3 * N, 1))\n",
    "    for i in range(N):\n",
    "        Ra, ta = A[i][0:3, 0:3], A[i][0:3, 3]\n",
    "        Rb, tb = B[i][0:3, 0:3], B[i][0:3, 3]\n",
    "        C[3 * i : 3 * i + 3, :] = np.eye(3) - Ra\n",
    "        d[3 * i : 3 * i + 3, 0] = ta - np.dot(Rx, tb)\n",
    "\n",
    "    tx = np.dot(np.linalg.inv(np.dot(C.T, C)), np.dot(C.T, d))\n",
    "    X = np.eye(4)\n",
    "    X[:3, :3] = Rx\n",
    "    X[:3, 3] = tx.flatten()\n",
    "    return X\n",
    "\n",
    "\n",
    "def calibrate_gripper_cam_peak_martin(T_robot_tcp_list, T_cam_marker_list):\n",
    "    ECs = []\n",
    "    for T_robot_tcp, t_cam_marker in zip(T_robot_tcp_list, T_cam_marker_list):\n",
    "        T_tcp_robot = np.linalg.inv(T_robot_tcp)\n",
    "        ECs.append((T_tcp_robot, t_cam_marker))\n",
    "\n",
    "    As = []  # relative EEs\n",
    "    Bs = []  # relative cams\n",
    "    for pair in itertools.combinations(ECs, 2):\n",
    "        (e_1, c_1), (e_2, c_2) = pair\n",
    "        A = e_2 @ np.linalg.inv(e_1)\n",
    "        B = c_2 @ np.linalg.inv(c_1)\n",
    "        As.append(A)\n",
    "        Bs.append(B)\n",
    "\n",
    "        # symmetrize\n",
    "        A = e_1 @ np.linalg.inv(e_2)\n",
    "        B = c_1 @ np.linalg.inv(c_2)\n",
    "        As.append(A)\n",
    "        Bs.append(B)\n",
    "\n",
    "    X = calibrate(As, Bs)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analysis of results\n",
    "\n",
    "Lets have a look at the results of both methods. \n",
    "\n",
    "1. How much does each datapoint contribute to the overall error? How much does each cartesian dimension contribute to the overall error?\n",
    "2. Does the error correlate with distance from the camera?\n",
    "3. How do the results change if we remove high error datapoints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "\n",
    "T_robot_tcp_list, T_cam_marker_list = get_tcp_marker_lists()\n",
    "\n",
    "t1 = default_timer()\n",
    "T_tcp_cam = calibrate_gripper_cam_ls(T_robot_tcp_list, T_cam_marker_list)\n",
    "print(f\"LS: {default_timer() - t1:.6f}s\")\n",
    "\n",
    "err_ls = pose_error(T_tcp_cam, T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls_s = np.sum(err_ls**2, axis=1)\n",
    "print(\"median error ls\", np.median(err_ls_s))\n",
    "\n",
    "t1 = default_timer()\n",
    "T_tcp_cam = calibrate_gripper_cam_peak_martin(T_robot_tcp_list, T_cam_marker_list)\n",
    "print(f\"PM: {default_timer() - t1:.6f}s\")\n",
    "\n",
    "err_pm = pose_error(T_tcp_cam, T_robot_tcp_list, T_cam_marker_list)\n",
    "err_pm_s = np.sum(err_pm**2, axis=1)\n",
    "print(\"median error pm\", np.median(err_pm_s))\n",
    "\n",
    "# timing measurement is slightly inaccurate but shows that PM is a bit faster.\n",
    "# to get more accurate measurements e.g. run the methods 100 times and average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO #################\n",
    "# 1. plot errors for each datapoint and method\n",
    "# 2. plot errors for each datapoint, method and axis\n",
    "raise NotImplementedError\n",
    "# END TODO ###################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error-Depth Correlation\n",
    "\n",
    "Does the error correlate with distance from the camera?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO #################\n",
    "raise NotImplementedError\n",
    "# END TODO ###################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Remove High-Error Samples.\n",
    "\n",
    "How do the results change if we remove high error datapoints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO #################\n",
    "raise NotImplementedError\n",
    "# END TODO ###################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Complete\n",
    "\n",
    "The additional material is provided as a quick introduction into Open3D. There is nothing to do there, run the code if you have time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally lets plot errors as a function of T_robot_tcp positions\n",
    "# This can be done using the open3d create_coordinate_frame command.\n",
    "mesh_frames = []\n",
    "\n",
    "# create one big frame to show the absolute zero point of the plot\n",
    "mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.25)\n",
    "mesh_frames.append(mesh_frame)\n",
    "\n",
    "# T_robot_tcp shows where the tool is relative to the robot, scale shows the error\n",
    "err_ls = pose_error(T_tcp_cam, T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls_s = np.sum(err_ls**2, axis=1)\n",
    "err_ls_sn = err_ls_s / np.sum(err_ls_s)\n",
    "for T_robot_tcp, err in zip(T_robot_tcp_list, err_ls_sn):\n",
    "    mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.05 + err)\n",
    "    mesh_frame.transform(T_robot_tcp)\n",
    "    mesh_frames.append(mesh_frame)\n",
    "o3d.visualization.draw_geometries(mesh_frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Merged Pointclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(o3d.visualization.draw_geometries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "def fuse_pointclouds(title, transform_fn):\n",
    "    first_rgb = Image.open(vl.get_rgb_file(1))\n",
    "    K_o3d = o3d.camera.PinholeCameraIntrinsic()\n",
    "    K_o3d.set_intrinsics(first_rgb.size[1], first_rgb.size[0], K[0, 0], K[1, 1], K[0, 2], K[1, 2])\n",
    "\n",
    "    pcd_list = []\n",
    "    for i in range(len(vl)):\n",
    "        try:\n",
    "            rgb_file = Image.open(vl.get_rgb_file(i))\n",
    "            depth_file = Image.open(vl.get_depth_file(i))\n",
    "            T_cam_marker = vl.get_cam_pose(i)\n",
    "            T_robot_tcp = vl.get_robot_pose(i)\n",
    "            depth_scaling = vl.get_robot_pose(i, return_dict=True)[1][\"depth_scaling\"]\n",
    "        except (FileNotFoundError, ValueError):\n",
    "            continue\n",
    "\n",
    "        # create pointcloud from RGBD data\n",
    "        rgb = o3d.geometry.Image(np.array(rgb_file))\n",
    "        depth = o3d.geometry.Image(np.array(depth_file).astype(np.uint16))\n",
    "        rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "            rgb, depth, depth_scale=1.0 / depth_scaling, depth_trunc=1.0, convert_rgb_to_intensity=False\n",
    "        )\n",
    "        pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd, K_o3d)\n",
    "\n",
    "        pcd = transform_fn(pcd, T_cam_marker, T_robot_tcp, T_tcp_cam)\n",
    "\n",
    "        pcd_list.append(pcd)\n",
    "\n",
    "    # sum pointclouds for easier visualization\n",
    "    pcd_all = pcd_list[0]\n",
    "    for pcd_cur in pcd_list:\n",
    "        pcd_all += pcd_cur\n",
    "\n",
    "    # create zero point\n",
    "    mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.2)\n",
    "    o3d.visualization.draw_geometries([pcd_all, mesh_frame], window_name=title)\n",
    "\n",
    "def transform_with_marker_cam(pcd, T_cam_marker, T_robot_tcp, T_tcp_cam):\n",
    "    # transform pointcloud from marker to camera coordinate system\n",
    "    # with original measurement\n",
    "    T_marker_cam = np.linalg.inv(T_cam_marker)\n",
    "    pcd.transform(T_marker_cam)\n",
    "    return pcd\n",
    "\n",
    "def transform_with_calibration(pcd, T_cam_marker, T_robot_tcp, T_tcp_cam):\n",
    "    # use calibrated transform\n",
    "    # we can ignore T_robot_marker because it is only a constant translation\n",
    "    T_robot_cam = T_robot_tcp @ T_tcp_cam\n",
    "    pcd.transform(T_robot_cam)\n",
    "    return pcd\n",
    "\n",
    "def no_transform(pcd, T_cam_marker, T_robot_tcp, T_tcp_cam):\n",
    "    return pcd\n",
    "\n",
    "# 1) transform the pointclouds into the camera space using the marker detection results\n",
    "fuse_pointclouds(\"Transform with marker detection result\", transform_with_marker_cam)\n",
    "\n",
    "# 2) transform the pointclouds into camera space using the calibration results\n",
    "# if the calibration worked out, this should give visually the same result as 1)\n",
    "# with the only difference that the zero point is not at the mesh origin\n",
    "# since we ignored the T_robot_marker translation here.\n",
    "fuse_pointclouds(\"Transform with calibration\", transform_with_calibration)\n",
    "\n",
    "# 3) do not transform the pointclouds. now the camera position is fixed\n",
    "# and the marker board moves around. we can see how the distance from\n",
    "# camera to marker board changes.\n",
    "fuse_pointclouds(\"No transform\", no_transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40de0b164a2ead70a1213ee87ce739cfc5594d2111c42683eb8f5e0739ba5537"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}